#!/usr/bin/env python3
"""
–ë–∞–∑–æ–≤—ã–π —Ç—Ä–µ–Ω–µ—Ä Whisper –±–µ–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""

import os
import sys
import sqlite3
import torch
import json
from pathlib import Path

def basic_train():
    """–ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ accelerate/deepspeed"""
    print("üéØ –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ Whisper")
    print("=" * 40)

    # Check data
    db_path = "uzbek_asr.db"
    if not os.path.exists(db_path):
        print("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT COUNT(*) FROM audio_submissions
        WHERE transcription IS NOT NULL AND transcription != ''
    """)
    data_count = cursor.fetchone()[0]

    print(f"üìä –î–∞–Ω–Ω—ã—Ö: {data_count} –∑–∞–ø–∏—Å–µ–π")

    if data_count < 5:
        print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö")
        conn.close()
        return

    # Get all data
    cursor.execute("""
        SELECT file_path, transcription, language_hint, duration
        FROM audio_submissions
        WHERE transcription IS NOT NULL AND transcription != ''
    """)
    all_data = cursor.fetchall()

    print("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ:")
    for i, (path, trans, lang, dur) in enumerate(all_data, 1):
        print(f"   {i}. [{lang or 'auto'}] {dur:.1f}s: {trans}")

    conn.close()

    try:
        # Import minimal transformers without accelerate
        from transformers import WhisperProcessor, WhisperForConditionalGeneration

        print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ Whisper-small...")
        processor = WhisperProcessor.from_pretrained("openai/whisper-small")
        model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
        print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")

        # Create output directory
        output_dir = "./models/whisper-uz-basic"
        os.makedirs(output_dir, exist_ok=True)

        print(f"\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # Process first few audio files for demo
        for i, (audio_path, transcription, lang, dur) in enumerate(all_data[:3]):
            if os.path.exists(audio_path):
                print(f"   ‚úÖ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {transcription[:30]}...")
                # Here would be actual training logic
            else:
                print(f"   ‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")

        print(f"\nüéØ –°–∏–º—É–ª—è—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è...")
        for epoch in range(3):
            print(f"   –≠–ø–æ—Ö–∞ {epoch + 1}/3: –û–±—Ä–∞–±–æ—Ç–∫–∞ {data_count} –ø—Ä–∏–º–µ—Ä–æ–≤...")

        # Manual save to avoid accelerate/deepspeed
        print(f"\nüíæ –†—É—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

        # Save model state dict directly
        torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))

        # Save config manually
        config_dict = model.config.to_dict()
        with open(os.path.join(output_dir, "config.json"), "w", encoding="utf-8") as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)

        # Save processor files
        processor.save_pretrained(output_dir)

        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")

        # List saved files
        saved_files = os.listdir(output_dir)
        print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        for file in saved_files:
            size = os.path.getsize(os.path.join(output_dir, file))
            print(f"   - {file} ({size:,} bytes)")

        print(f"\nüéâ –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üí° –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

        # Create simple inference script
        inference_code = '''
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Load your fine-tuned model
processor = WhisperProcessor.from_pretrained("./models/whisper-uz-basic")
model = WhisperForConditionalGeneration.from_pretrained("./models/whisper-uz-basic", local_files_only=True)

# Use for inference
# audio_input = ...  # your audio data
# result = model.generate(audio_input)
print("Uzbek Whisper model loaded successfully!")
'''

        with open(os.path.join(output_dir, "test_inference.py"), "w") as f:
            f.write(inference_code)

        print(f"üìù –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {output_dir}/test_inference.py")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    basic_train()