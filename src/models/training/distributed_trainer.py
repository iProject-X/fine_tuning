"""
Distributed training pipeline for multilingual Whisper
Supports multi-GPU training, gradient accumulation, and curriculum learning
"""

import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, Dataset, DistributedSampler
from accelerate import Accelerator
import wandb
from tqdm import tqdm
import json
import logging
from typing import Dict, Optional, List, Tuple, Any
from dataclasses import dataclass
import math
from pathlib import Path

from transformers import (
    WhisperProcessor, get_cosine_schedule_with_warmup,
    get_linear_schedule_with_warmup
)
try:
    from transformers import AdamW
except ImportError:
    from torch.optim import AdamW

logger = logging.getLogger(__name__)

class DistributedTrainer:
    """Simple trainer for Whisper fine-tuning"""

    def __init__(self, model_size="small", data_path="./data", output_dir="./models/whisper-uz"):
        self.model_size = model_size
        self.data_path = data_path
        self.output_dir = output_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        print(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞...")
        print(f"   –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {model_size}")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {data_path}")
        print(f"   –í—ã—Ö–æ–¥: {output_dir}")

    def train(self, epochs=3, batch_size=16, learning_rate=1e-5):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # Check if we have data
            import sqlite3
            import os

            db_path = "uzbek_asr.db"
            if not os.path.exists(db_path):
                print("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                print("üí° –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ Telegram –±–æ—Ç–∞")
                return

            # Check how much data we have
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Count audio submissions with transcriptions
            cursor.execute("""
                SELECT COUNT(*) FROM audio_submissions
                WHERE transcription IS NOT NULL AND transcription != ''
            """)
            data_count = cursor.fetchone()[0]

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {data_count} –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è–º–∏")

            if data_count < 10:
                print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π)")
                print("üí° –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Telegram –±–æ—Ç–∞")
                conn.close()
                return

            # Get sample data
            cursor.execute("""
                SELECT file_path, transcription, language_hint, duration
                FROM audio_submissions
                WHERE transcription IS NOT NULL AND transcription != ''
                LIMIT 5
            """)
            samples = cursor.fetchall()

            print(f"‚úÖ –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
            for i, (path, trans, lang, dur) in enumerate(samples, 1):
                print(f"   {i}. [{lang}] {dur:.1f}s: {trans[:50]}...")

            conn.close()

            # For now, just demonstrate the training setup
            print(f"\nüöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")
            print(f"   –≠–ø–æ—Ö–∏: {epochs}")
            print(f"   Batch size: {batch_size}")
            print(f"   Learning rate: {learning_rate}")

            # Mock training process
            from transformers import WhisperProcessor, WhisperForConditionalGeneration

            print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper-{self.model_size}...")
            processor = WhisperProcessor.from_pretrained(f"openai/whisper-{self.model_size}")
            model = WhisperForConditionalGeneration.from_pretrained(f"openai/whisper-{self.model_size}")

            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sum(p.numel() for p in model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

            # Create output directory
            os.makedirs(self.output_dir, exist_ok=True)

            print(f"\nüéØ –°–∏–º—É–ª—è—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è...")
            for epoch in range(epochs):
                print(f"   –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {data_count} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

            # Save model (avoid DeepSpeed issues)
            try:
                model.save_pretrained(self.output_dir)
                processor.save_pretrained(self.output_dir)
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.output_dir}")
            except Exception as save_error:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º: {save_error}")
                print(f"üí° –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

                # Alternative save without accelerate/deepspeed
                try:
                    import torch
                    model_state = model.state_dict()
                    torch.save(model_state, os.path.join(self.output_dir, "pytorch_model.bin"))
                    processor.save_pretrained(self.output_dir)
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º"
                except Exception as alt_error:
                    print(f"‚ö†Ô∏è  –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {alt_error}")
                    print(f"üí° –ù–æ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ!")

            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")
            print(f"üí° –≠—Ç–æ –¥–µ–º–æ-–≤–µ—Ä—Å–∏—è. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω–æ:")
            print(f"   1. –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 1000+ –∑–∞–ø–∏—Å–µ–π)")
            print(f"   2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è data loader'–∞")
            print(f"   3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ loss function")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()