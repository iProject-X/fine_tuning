#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Uzbek ASR
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import os

class DataAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö ASR"""

    def __init__(self, db_path="uzbek_asr.db"):
        self.db_path = db_path

        print("üìä –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö Uzbek ASR")
        print("=" * 40)

    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î"""
        if not os.path.exists(self.db_path):
            print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {self.db_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None

        conn = sqlite3.connect(self.db_path)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –∑–∞–ø–∏—Å—è—Ö
        query = """
        SELECT
            id,
            user_id,
            file_path,
            duration,
            language_hint,
            transcription,
            quality_score,
            verification_count,
            verified,
            timestamp
        FROM audio_submissions
        WHERE transcription IS NOT NULL AND transcription != ''
        ORDER BY timestamp
        """

        df = pd.read_sql_query(query, conn)
        conn.close()

        print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        return df

    def analyze_quality_distribution(self, df):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"""
        print("\nüéØ –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•")
        print("-" * 30)

        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ quality_score:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {df['quality_score'].mean():.3f}")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞: {df['quality_score'].median():.3f}")
        print(f"   –ú–∏–Ω–∏–º—É–º: {df['quality_score'].min():.3f}")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {df['quality_score'].max():.3f}")
        print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {df['quality_score'].std():.3f}")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        df['quality_category'] = pd.cut(df['quality_score'],
                                       bins=[0, 0.3, 0.6, 0.8, 1.0],
                                       labels=['–ù–∏–∑–∫–æ–µ', '–°—Ä–µ–¥–Ω–µ–µ', '–•–æ—Ä–æ—à–µ–µ', '–û—Ç–ª–∏—á–Ω–æ–µ'])

        quality_counts = df['quality_category'].value_counts()
        print(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, count in quality_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   {category}: {count} ({percentage:.1f}%)")

        return df

    def analyze_content(self, df):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        print("\nüìù –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ê–ù–ò–Ø")
        print("-" * 25)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print(f"‚è±Ô∏è  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ:")
        print(f"   –°—Ä–µ–¥–Ω—è—è: {df['duration'].mean():.1f} —Å–µ–∫")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞: {df['duration'].median():.1f} —Å–µ–∫")
        print(f"   –ú–∏–Ω–∏–º—É–º: {df['duration'].min():.1f} —Å–µ–∫")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {df['duration'].max():.1f} —Å–µ–∫")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        df['text_length'] = df['transcription'].str.len()
        print(f"\nüìè –î–ª–∏–Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π:")
        print(f"   –°—Ä–µ–¥–Ω—è—è: {df['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞: {df['text_length'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∏–Ω–∏–º—É–º: {df['text_length'].min()} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {df['text_length'].max()} —Å–∏–º–≤–æ–ª–æ–≤")

        # –ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤
        if 'language_hint' in df.columns:
            lang_counts = df['language_hint'].value_counts()
            print(f"\nüåê –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º:")
            for lang, count in lang_counts.items():
                percentage = (count / len(df)) * 100
                print(f"   {lang or '–ù–µ —É–∫–∞–∑–∞–Ω'}: {count} ({percentage:.1f}%)")

        return df

    def show_best_samples(self, df, n=5):
        """–ü–æ–∫–∞–∑–∞—Ç—å –ª—É—á—à–∏–µ –æ–±—Ä–∞–∑—Ü—ã"""
        print(f"\n‚≠ê –¢–û–ü-{n} –û–ë–†–ê–ó–¶–û–í –ü–û –ö–ê–ß–ï–°–¢–í–£")
        print("-" * 35)

        best_samples = df.nlargest(n, 'quality_score')

        for i, (_, row) in enumerate(best_samples.iterrows(), 1):
            print(f"{i}. Quality: {row['quality_score']:.3f} | "
                  f"Duration: {row['duration']:.1f}s | "
                  f"Lang: {row['language_hint'] or 'auto'}")
            print(f"   üìù {row['transcription']}")
            print()

    def show_worst_samples(self, df, n=3):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ö—É–¥—à–∏–µ –æ–±—Ä–∞–∑—Ü—ã"""
        print(f"\n‚ö†Ô∏è  –û–ë–†–ê–ó–¶–´ –° –ù–ò–ó–ö–ò–ú –ö–ê–ß–ï–°–¢–í–û–ú (–¢–û–ü-{n})")
        print("-" * 40)

        worst_samples = df.nsmallest(n, 'quality_score')

        for i, (_, row) in enumerate(worst_samples.iterrows(), 1):
            print(f"{i}. Quality: {row['quality_score']:.3f} | "
                  f"Duration: {row['duration']:.1f}s | "
                  f"Lang: {row['language_hint'] or 'auto'}")
            print(f"   üìù {row['transcription']}")
            print()

    def recommend_training_strategy(self, df):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        print("\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–ë–£–ß–ï–ù–ò–Æ")
        print("-" * 35)

        total_samples = len(df)
        high_quality = len(df[df['quality_score'] >= 0.7])
        medium_quality = len(df[(df['quality_score'] >= 0.5) & (df['quality_score'] < 0.7)])
        low_quality = len(df[df['quality_score'] < 0.5])

        print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (‚â•0.7): {high_quality} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (0.5-0.7): {medium_quality} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"   –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (<0.5): {low_quality} –æ–±—Ä–∞–∑—Ü–æ–≤")

        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")

        if total_samples < 50:
            print("   üìà –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 100+ –æ–±—Ä–∞–∑—Ü–æ–≤)")
        elif total_samples < 200:
            print("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        else:
            print("   üéâ –û—Ç–ª–∏—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")

        if high_quality < total_samples * 0.5:
            print("   üîß –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (quality_score >= 0.6)")

        if low_quality > total_samples * 0.3:
            print("   ‚ö†Ô∏è  –ú–Ω–æ–≥–æ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –æ–±—É—á–µ–Ω–∏—è
        print(f"\n‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        if total_samples <= 50:
            print("   Epochs: 3-5, Batch size: 8, Learning rate: 1e-5")
        elif total_samples <= 200:
            print("   Epochs: 5-8, Batch size: 16, Learning rate: 5e-6")
        else:
            print("   Epochs: 8-15, Batch size: 32, Learning rate: 1e-6")

    def export_analysis(self, df):
        """–≠–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"\nüíæ –≠–ö–°–ü–û–†–¢ –ê–ù–ê–õ–ò–ó–ê")
        print("-" * 20)

        # –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        export_dir = Path("./analysis_results")
        export_dir.mkdir(exist_ok=True)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            "total_samples": len(df),
            "quality_stats": {
                "mean": float(df['quality_score'].mean()),
                "median": float(df['quality_score'].median()),
                "min": float(df['quality_score'].min()),
                "max": float(df['quality_score'].max()),
                "std": float(df['quality_score'].std())
            },
            "duration_stats": {
                "mean": float(df['duration'].mean()),
                "median": float(df['duration'].median()),
                "min": float(df['duration'].min()),
                "max": float(df['duration'].max()),
                "total_hours": float(df['duration'].sum() / 3600)
            },
            "quality_distribution": df['quality_category'].value_counts().to_dict(),
            "language_distribution": df['language_hint'].value_counts().to_dict() if 'language_hint' in df.columns else {},
            "best_samples": df.nlargest(5, 'quality_score')[['transcription', 'quality_score', 'duration']].to_dict('records'),
            "recommendations": self._get_recommendations(df)
        }

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON
        with open(export_dir / "analysis_report.json", "w", encoding="utf-8") as f:
            json.dump(stats, f, indent=2, ensure_ascii=False, default=str)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å CSV
        df.to_csv(export_dir / "data_samples.csv", index=False, encoding="utf-8")

        print(f"   ‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {export_dir}/analysis_report.json")
        print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {export_dir}/data_samples.csv")

    def _get_recommendations(self, df):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        total = len(df)
        high_quality = len(df[df['quality_score'] >= 0.7])

        recommendations = []

        if total < 50:
            recommendations.append("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
        if high_quality < total * 0.5:
            recommendations.append("–£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∏")
        if df['duration'].mean() < 3:
            recommendations.append("–ó–∞–ø–∏—Å—ã–≤–∞–π—Ç–µ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã")

        return recommendations

    def run_analysis(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        df = self.load_data()
        if df is None:
            return

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑
        df = self.analyze_quality_distribution(df)
        df = self.analyze_content(df)
        self.show_best_samples(df)
        self.show_worst_samples(df)
        self.recommend_training_strategy(df)
        self.export_analysis(df)

        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return df

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö Uzbek ASR...")

    analyzer = DataAnalyzer()
    analyzer.run_analysis()

if __name__ == "__main__":
    main()